{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归的简洁实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 生成数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from d2l import torch as d2l\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = torch.tensor(4.2)\n",
    "features, labels = d2l.synthetic_data(true_w,true_b,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 读取数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以调用框架中现有的API来读取数据。 我们将features和labels作为API的参数传递，并通过数据迭代器指定batch_size。 此外，布尔值is_train表示是否希望数据迭代器对象在每个迭代周期内打乱数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_array(data_arrays, batch_size, is_train = True):   #@save\n",
    "    \"\"\"构造一个PyTorch数据迭代器\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 1.6202,  0.4736],\n",
       "         [ 0.2814, -0.3443],\n",
       "         [-0.4907, -0.1302],\n",
       "         [-0.5667, -1.3578],\n",
       "         [ 0.0396, -1.4646],\n",
       "         [ 0.2695, -1.0965],\n",
       "         [-0.5189,  0.5914],\n",
       "         [ 1.0722, -0.2794],\n",
       "         [-0.5667, -0.2096],\n",
       "         [ 0.4675,  1.1573]]),\n",
       " tensor([[5.8381],\n",
       "         [5.9347],\n",
       "         [3.6553],\n",
       "         [7.6949],\n",
       "         [9.2546],\n",
       "         [8.4559],\n",
       "         [1.1368],\n",
       "         [7.3030],\n",
       "         [3.7762],\n",
       "         [1.1933]])]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "net = nn.Sequential(nn.Linear(2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 初始化模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度学习框架通常有预定义的方法来初始化参数。 在这里，我们指定每个权重参数应该从均值为0、标准差为0.01的正态分布中随机采样， 偏置参数将初始化为零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.normal_(0, 0.01)\n",
    "net[0].bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 定义优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(net.parameters(), lr = 0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 对于每一个小批量，我们会进行以下步骤:\n",
    "\n",
    "- 通过调用net(X)生成预测并计算损失l（前向传播）。\n",
    "\n",
    "- 通过进行反向传播来计算梯度。\n",
    "\n",
    "- 通过调用优化器来更新模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss:0.00020\n",
      "epoch:1, loss:0.00010\n",
      "epoch:2, loss:0.00010\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        optim.zero_grad()\n",
    "        l.backward()\n",
    "        optim.step()\n",
    "    #计算总的loss\n",
    "    with torch.no_grad():\n",
    "        l = loss(net(features),labels)\n",
    "        print(f'epoch:{epoch}, loss:{l:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的估计误差:  [0.00015079975128173828, 0.00026297569274902344]\n",
      "b的估计误差:  [0.000514984130859375]\n"
     ]
    }
   ],
   "source": [
    "w = net[0].weight.data\n",
    "b = net[0].bias.data\n",
    "print('w的估计误差: ',(true_w - w).flatten().tolist())\n",
    "print('b的估计误差: ',(true_b - b).tolist())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "72218599f3e462b6cfb0374e23867e8f4de18e0d1a645225b63f6921d26b1799"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
